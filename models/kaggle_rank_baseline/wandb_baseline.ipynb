{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "class MarkdownDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_path: str = None, batch_size: int = 32,\n",
    "                 train_dat=None, val_dat=None, model = \"distilbert-base-uncased\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.train_path = train_path\n",
    "        self.batch_size = batch_size\n",
    "        self.validation_size = 0.1\n",
    "        self.padding = 128\n",
    "\n",
    "        self.tokenizer = DistilBertTokenizer.from_pretrained(model, do_lower_case=True)\n",
    "\n",
    "        self.train_dataset, self.val_dataset = train_dat, val_dat\n",
    "\n",
    "    def _preprocess_dataset(self, df):\n",
    "\n",
    "        dataset = Dataset.from_pandas(df)\n",
    "\n",
    "        def process_batch(batch):\n",
    "            tokenized = self.tokenizer(\n",
    "                batch['source'],\n",
    "                padding='max_length',\n",
    "                truncation=True,\n",
    "                max_length=self.padding\n",
    "            )\n",
    "            return tokenized\n",
    "\n",
    "        dataset = dataset.map(\n",
    "            lambda batch: process_batch(batch),\n",
    "            batched=True, batch_size=self.batch_size,\n",
    "        )\n",
    "\n",
    "        dataset.set_format('pt', ['input_ids', 'attention_mask', 'score'])\n",
    "\n",
    "        return dataset\n",
    "\n",
    "    def _split_if_ancestors(self, df):\n",
    "\n",
    "        splitter = GroupShuffleSplit(n_splits=1, test_size=self.validation_size, random_state=0)\n",
    "        train_ind, val_ind = next(splitter.split(df, groups=df[\"ancestor_id\"]))\n",
    "        train_df, val_df = df.loc[train_ind].reset_index(drop=True), df.loc[val_ind].reset_index(drop=True)\n",
    "\n",
    "        return train_df, val_df\n",
    "\n",
    "\n",
    "    def _prepare_dataset(self):\n",
    "        if (self.train_dataset is not None) and (self.val_dataset is not None):\n",
    "            return\n",
    "\n",
    "        df = pd.read_feather(self.train_path)\n",
    "        df = df.rename(columns = {'pct_rank':'score'})\n",
    "        # Add data cleaninng step\n",
    "        train_df, val_df = self._split_if_ancestors(df)\n",
    "        print('prepearing train data')\n",
    "        self.train_dataset = self._preprocess_dataset(train_df)\n",
    "        print('prepearing test data')\n",
    "        self.val_dataset = self._preprocess_dataset(val_df)\n",
    "\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self._prepare_dataset()\n",
    "\n",
    "        if stage == 'test' or stage is None:\n",
    "            pass\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset,\n",
    "                          batch_size=self.batch_size, num_workers=4,\n",
    "                          pin_memory=True, shuffle=True)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset,\n",
    "                          batch_size=self.batch_size, num_workers=4,\n",
    "                          pin_memory=True)\n",
    "\n",
    "\n",
    "class MarkdownModelPl(pl.LightningModule):\n",
    "    def __init__(self, model = \"distilbert-base-uncased\"):\n",
    "        super(MarkdownModelPl, self).__init__()\n",
    "\n",
    "        self.distill_bert = DistilBertModel.from_pretrained(model, return_dict=True)\n",
    "        self.dense = torch.nn.Linear(768, 1)\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        self.activation = torch.nn.LeakyReLU()\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, score):\n",
    "\n",
    "        embeddings = self.distill_bert(input_ids, attention_mask)['last_hidden_state']\n",
    "        embeddings = self.activation(embeddings)\n",
    "        preds = self.dense(embeddings[:, 0, :]) # why are you taking embeding of first token, maybe mean?\n",
    "\n",
    "        return preds\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        preds = self.forward(**batch).reshape(-1)\n",
    "        loss = self.loss(preds, batch['score'])\n",
    "        self.log('train_batch_loss', loss)\n",
    "        self.log('train_RMSE',1)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        preds = self.forward(**batch).reshape(-1)\n",
    "        loss = self.loss(preds, batch['score'])\n",
    "\n",
    "        self.log('val_batch_loss', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(\n",
    "            filter(lambda p: p.requires_grad, self.parameters()),\n",
    "            lr=3e-4, betas=(0.9, 0.999), eps=1e-08)\n",
    "        return optimizer"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = \"distilbert-base-uncased\"\n",
    "data_path = 'data/ranks.fth'\n",
    "# df = pd.read_feather(data_path)\n",
    "MDM = MarkdownDataModule(data_path)\n",
    "model = MarkdownModelPl()\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"JupyterKaggleBaseline\")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus = 1,\n",
    "    max_epochs=20,\n",
    "    logger=wandb_logger,\n",
    "    enable_progress_bar=False,\n",
    "    log_every_n_steps=20,\n",
    "    accumulate_grad_batches=4,\n",
    ")\n",
    "trainer.fit(model, MDM)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating submission"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_path = '../input/train-markdown-ranks/test_dataset.fth'\n",
    "test_df = pd.read_feather(test_path)\n",
    "test_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df[\"pct_rank\"] = 0\n",
    "test_ds = MarkdownDataset(\n",
    "    test_df[test_df[\"cell_type\"] == \"markdown\"].reset_index(drop=True), max_len=MAX_LEN\n",
    ")\n",
    "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=1,\n",
    "                          pin_memory=False, drop_last=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "_, y_test = validate(model, test_loader)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_df.loc[test_df[\"cell_type\"] == \"markdown\", \"pred\"] = y_test\n",
    "sub_df = test_df.sort_values(\"pred\").groupby(\"id\")[\"cell_id\"].apply(lambda x: \" \".join(x)).reset_index()\n",
    "sub_df.rename(columns={\"cell_id\": \"cell_order\"}, inplace=True)\n",
    "sub_df.head()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"submission.csv\", index=False)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}